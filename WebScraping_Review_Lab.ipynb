{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Web Scraping Lab**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ul>\n",
    "        <li>\n",
    "            <a href=\"https://bso/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01\">Beautiful Soup Object</a>\n",
    "            <ul>\n",
    "                <li>Tag</li>\n",
    "                <li>Children, Parents, and Siblings</li>\n",
    "                <li>HTML Attributes</li>\n",
    "                <li>Navigable String</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "     </ul>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <a href=\"https://filter/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01\">Filter</a>\n",
    "            <ul>\n",
    "                <li>find All</li>\n",
    "                <li>find </li>\n",
    "                <li>HTML Attributes</li>\n",
    "                <li>Navigable String</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "     </ul>\n",
    "     <ul>\n",
    "        <li>\n",
    "            <a href=\"https://dscw/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01\">Downloading And Scraping The Contents Of A Web</a>\n",
    "    </li>\n",
    "         </ul>\n",
    "    <p>\n",
    "        Estimated time needed: <strong>25 min</strong>\n",
    "    </p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we are going to be using Python and several Python libraries. Some of these libraries might be installed in your lab environment or in SN Labs. Others may need to be installed by you. The cells below will install these libraries when executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 24.1.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.1.2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml==4.6.4\n",
      "  Using cached lxml-4.6.4.tar.gz (3.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: lxml\n",
      "  Building wheel for lxml (setup.py): started\n",
      "  Building wheel for lxml (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for lxml\n",
      "Failed to build lxml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [76 lines of output]\n",
      "  Building lxml version 4.6.4.\n",
      "  C:\\Users\\SaMBesT PC\\AppData\\Local\\Temp\\pip-install-k5bj7vlc\\lxml_49ea186a5f6f43a984dc8624f07e431f\\setup.py:67: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "  Building without Cython.\n",
      "  Building against pre-built libxml2 andl libxslt libraries\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\builder.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\cssselect.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\doctestcompare.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\ElementInclude.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\pyclasslookup.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\sax.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\usedoctest.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\_elementpath.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\builder.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\clean.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\defs.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\diff.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\ElementSoup.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\formfill.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\html5parser.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\soupparser.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\usedoctest.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\_diffcommand.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\_html5builder.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\_setmixin.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\n",
      "  copying src\\lxml\\isoschematron\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\n",
      "  copying src\\lxml\\etree.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\etree_api.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\lxml.etree.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\lxml.etree_api.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\includes\\c14n.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\config.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\dtdvalid.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\etreepublic.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\htmlparser.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\relaxng.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\schematron.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\tree.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\uri.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xinclude.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlerror.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlparser.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlschema.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xpath.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xslt.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\__init__.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\etree_defs.h -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\lxml-version.h -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\rng\n",
      "  copying src\\lxml\\isoschematron\\resources\\rng\\iso-schematron.rng -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\rng\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\RNG2Schtrn.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\XSD2Schtrn.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_abstract_expand.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_dsdl_include.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_message.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_skeleton_for_xslt1.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_svrl_for_xslt1.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\readme.txt -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  running build_ext\n",
      "  building 'lxml.etree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for lxml\n",
      "ERROR: Could not build wheels for lxml, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 24.1.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.1.2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install bs4==4.10.0 -y\n",
    "!pip install lxml==4.6.4\n",
    "!conda install html5lib==1.1 -y\n",
    "# !pip install requests==2.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # this module helps in web scrapping.\n",
    "import requests  # this module helps us to download a web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"BSO\">Beautiful Soup Objects</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files, we will focus on HTML files. This is accomplished by representing the HTML as a set of objects with methods used to parse the HTML.  We can navigate the HTML as a tree and/or filter out what we are looking for.\n",
    "\n",
    "Consider the following HTML:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "<title>Page Title</title>\n",
       "</head>\n",
       "<body>\n",
       "<h3><b id='boldest'>Lebron James</b></h3>\n",
       "<p> Salary: $ 92,000,000 </p>\n",
       "<h3> Stephen Curry</h3>\n",
       "<p> Salary: $85,000, 000 </p>\n",
       "<h3> Kevin Durant </h3>\n",
       "<p> Salary: $73,200, 000</p>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "<h3><b id='boldest'>Lebron James</b></h3>\n",
    "<p> Salary: $ 92,000,000 </p>\n",
    "<h3> Stephen Curry</h3>\n",
    "<p> Salary: $85,000, 000 </p>\n",
    "<h3> Kevin Durant </h3>\n",
    "<p> Salary: $73,200, 000</p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store it as a string in the variable HTML:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=\"<!DOCTYPE html><html><head><title>Page Title</title></head><body><h3><b id='boldest'>Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body></html>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse a document, pass it into the <code>BeautifulSoup</code> constructor, the <code>BeautifulSoup</code> object, which represents the document as a nested data structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the document is converted to Unicode, (similar to ASCII),  and HTML entities are converted to Unicode characters. Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. The <code>BeautifulSoup</code> object can create other types of objects. In this lab, we will cover <code>BeautifulSoup</code> and <code>Tag</code> objects that for the purposes of this lab are identical, and <code>NavigableString</code> objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the method <code>prettify()</code> to display the HTML in the nested structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Page Title\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h3>\n",
      "   <b id=\"boldest\">\n",
      "    Lebron James\n",
      "   </b>\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $ 92,000,000\n",
      "  </p>\n",
      "  <h3>\n",
      "   Stephen Curry\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $85,000, 000\n",
      "  </p>\n",
      "  <h3>\n",
      "   Kevin Durant\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $73,200, 000\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want the  title of the page and the name of the top paid player we can use the <code>Tag</code>. The <code>Tag</code> object corresponds to an HTML tag in the original document, for example, the tag title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag object: <title>Page Title</title>\n"
     ]
    }
   ],
   "source": [
    "tag_object=soup.title\n",
    "print(\"tag object:\",tag_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the tag type <code>bs4.element.Tag</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag object type: <class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(\"tag object type:\",type(tag_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is more than one <code>Tag</code>  with the same name, the first element with that <code>Tag</code> name is called, this corresponds to the most paid player:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3><b id=\"boldest\">Lebron James</b></h3>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_object=soup.h3\n",
    "tag_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enclosed in the bold attribute <code>b</code>, it helps to use the tree representation. We can navigate down the tree using the child attribute to get the name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Children, Parents, and Siblings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above the <code>Tag</code> object is a tree of objects we can access the child of the tag or navigate down the branch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b id=\"boldest\">Lebron James</b>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_child =tag_object.b\n",
    "tag_child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the parent with the <code> parent</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3><b id=\"boldest\">Lebron James</b></h3>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_tag=tag_child.parent\n",
    "parent_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is identical to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>tag_object</code> parent is the <code>body</code> element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body><h3><b id=\"boldest\">Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_object.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>tag_object</code> sibling is the <code>paragraph</code> element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p> Salary: $ 92,000,000 </p>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sibling_1=tag_object.next_sibling\n",
    "sibling_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sibling_2` is the `header` element which is also a sibling of both `sibling_1` and `tag_object`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3> Stephen Curry</h3>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sibling_2=sibling_1.next_sibling\n",
    "sibling_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"first_question\">Exercise: <code>next_sibling</code></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the object <code>sibling\\_2</code> and the property <code>next_sibling</code> to find the salary of Stephen Curry:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```\n",
    "sibling_2.next_sibling\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tag has attributes, the tag <code>id=\"boldest\"</code> has an attribute <code>id</code> whose value is <code>boldest</code>. You can access a tag’s attributes by treating the tag like a dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boldest'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_child['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access that dictionary directly as <code>attrs</code>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'id': 'boldest'}\n"
     ]
    }
   ],
   "source": [
    "print (soup.h3.attrs)\n",
    "print (soup.b.attrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'boldest'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_child.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also work with Multi-valued attribute check out <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01\">\\[1]</a> for more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain the content if the attribute of the <code>tag</code> using the Python <code>get()</code> method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boldest'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_child.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boldest\n"
     ]
    }
   ],
   "source": [
    "print (soup.b.get('id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigable String\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string corresponds to a bit of text or content within a tag. Beautiful Soup uses the <code>NavigableString</code> class to contain this text. In our HTML we can obtain the name of the first player by extracting the sting of the <code>Tag</code> object <code>tag_child</code> as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lebron James'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_string=tag_child.string\n",
    "tag_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can verify the type is Navigable String\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.NavigableString"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tag_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A NavigableString is just like a Python string or Unicode string, to be more precise. The main difference is that it also supports some  <code>BeautifulSoup</code> features. We can covert it to sting object in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_string = str(tag_string)\n",
    "unicode_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"filter\">Filter</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters allow you to find complex patterns, the simplest filter is a string. In this section we will pass a string to a different filter method and Beautiful Soup will perform a match against that exact string.  Consider the following HTML of rocket launchs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr>\n",
       "    <td id='flight' >Flight No</td>\n",
       "    <td>Launch site</td> \n",
       "    <td>Payload mass</td>\n",
       "   </tr>\n",
       "  <tr> \n",
       "    <td>1</td>\n",
       "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n",
       "    <td>300 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>2</td>\n",
       "    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n",
       "    <td>94 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>3</td>\n",
       "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td>\n",
       "    <td>80 kg</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<table>\n",
    "  <tr>\n",
    "    <td id='flight' >Flight No</td>\n",
    "    <td>Launch site</td> \n",
    "    <td>Payload mass</td>\n",
    "   </tr>\n",
    "  <tr> \n",
    "    <td>1</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n",
    "    <td>300 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n",
    "    <td>94 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td>\n",
    "    <td>80 kg</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store it as a string in the variable <code>table</code>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=\"<table><tr><td id='flight' >Flight No</td><td>Launch site</td><td>Payload mass</td></tr><tr><td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td><td>80 kg</td></tr></table>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bs = BeautifulSoup(table, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr><td id=\"flight\">Flight No</td><td>Launch site</td><td>Payload mass</td></tr>,\n",
       " <tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td><td>300 kg</td></tr>,\n",
       " <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n",
       " <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td><td>80 kg</td></tr>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_bs.find_all (\"tr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find All\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '’' (U+2019) (2574464483.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    The <code>find_all()</code> method looks through a tag’s descendants and retrieves all descendants that match your filters.\u001b[0m\n\u001b[1;37m                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '’' (U+2019)\n"
     ]
    }
   ],
   "source": [
    "The <code>find_all()</code> method looks through a tag’s descendants and retrieves all descendants that match your filters.\n",
    "\n",
    "<p>\n",
    "The Method signature for <code>find_all(name, attrs, recursive, string, limit, **kwargs)<c/ode>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we set the <code>name</code> parameter to a tag name, the method will extract all the tags with that name and its children.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr><td id=\"flight\">Flight No</td><td>Launch site</td><td>Payload mass</td></tr>,\n",
       " <tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td><td>300 kg</td></tr>,\n",
       " <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n",
       " <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td><td>80 kg</td></tr>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_rows=table_bs.find_all('tr')\n",
    "table_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a Python Iterable just like a list, each element is a <code>tag</code> object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr><td id=\"flight\">Flight No</td><td>Launch site</td><td>Payload mass</td></tr>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row =table_rows[0]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type is <code>tag</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(type(first_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can obtain the child\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td id=\"flight\">Flight No</td>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row.td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we iterate through the list, each element corresponds to a row in the table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0 is <tr><td id=\"flight\">Flight No</td><td>Launch site</td><td>Payload mass</td></tr>\n",
      "row 1 is <tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td><td>300 kg</td></tr>\n",
      "row 2 is <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>\n",
      "row 3 is <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td><td>80 kg</td></tr>\n"
     ]
    }
   ],
   "source": [
    "for i,row in enumerate(table_rows):\n",
    "    print(\"row\",i,\"is\",row)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As <code>row</code> is a <code>cell</code> object, we can apply the method <code>find_all</code> to it and extract table cells in the object <code>cells</code> using the tag <code>td</code>, this is all the children with the name <code>td</code>. The result is a list, each element corresponds to a cell and is a <code>Tag</code> object, we can iterate through this list as well. We can extract the content using the <code>string</code>  attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0\n",
      "colunm 0 cell <td id=\"flight\">Flight No</td>\n",
      "colunm 1 cell <td>Launch site</td>\n",
      "colunm 2 cell <td>Payload mass</td>\n",
      "row 1\n",
      "colunm 0 cell <td>1</td>\n",
      "colunm 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td>\n",
      "colunm 2 cell <td>300 kg</td>\n",
      "row 2\n",
      "colunm 0 cell <td>2</td>\n",
      "colunm 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>\n",
      "colunm 2 cell <td>94 kg</td>\n",
      "row 3\n",
      "colunm 0 cell <td>3</td>\n",
      "colunm 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td>\n",
      "colunm 2 cell <td>80 kg</td>\n"
     ]
    }
   ],
   "source": [
    "for i,row in enumerate(table_rows):\n",
    "    print(\"row\",i)\n",
    "    cells=row.find_all('td')\n",
    "    for j,cell in enumerate(cells):\n",
    "        print('colunm',j,\"cell\",cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a list we can match against any item in that list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr><td id=\"flight\">Flight No</td><td>Launch site</td><td>Payload mass</td></tr>,\n",
       " <td id=\"flight\">Flight No</td>,\n",
       " <td>Launch site</td>,\n",
       " <td>Payload mass</td>,\n",
       " <tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td><td>300 kg</td></tr>,\n",
       " <td>1</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a></td>,\n",
       " <td>300 kg</td>,\n",
       " <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n",
       " <td>2</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>,\n",
       " <td>94 kg</td>,\n",
       " <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td><td>80 kg</td></tr>,\n",
       " <td>3</td>,\n",
       " <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a> </td>,\n",
       " <td>80 kg</td>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_input=table_bs .find_all(name=[\"tr\", \"td\"])\n",
    "list_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table>\n",
      " <tr>\n",
      "  <td id=\"flight\">\n",
      "   Flight No\n",
      "  </td>\n",
      "  <td>\n",
      "   Launch site\n",
      "  </td>\n",
      "  <td>\n",
      "   Payload mass\n",
      "  </td>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td>\n",
      "   1\n",
      "  </td>\n",
      "  <td>\n",
      "   <a href=\"https://en.wikipedia.org/wiki/Florida\">\n",
      "    Florida\n",
      "   </a>\n",
      "  </td>\n",
      "  <td>\n",
      "   300 kg\n",
      "  </td>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td>\n",
      "   2\n",
      "  </td>\n",
      "  <td>\n",
      "   <a href=\"https://en.wikipedia.org/wiki/Texas\">\n",
      "    Texas\n",
      "   </a>\n",
      "  </td>\n",
      "  <td>\n",
      "   94 kg\n",
      "  </td>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td>\n",
      "   3\n",
      "  </td>\n",
      "  <td>\n",
      "   <a href=\"https://en.wikipedia.org/wiki/Florida\">\n",
      "    Florida\n",
      "   </a>\n",
      "  </td>\n",
      "  <td>\n",
      "   80 kg\n",
      "  </td>\n",
      " </tr>\n",
      "</table>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (table_bs.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the argument is not recognized it will be turned into a filter on the tag’s attributes. For example the <code>id</code>  argument, Beautiful Soup will filter against each tag’s <code>id</code> attribute. For example, the first <code>td</code> elements have a value of <code>id</code> of <code>flight</code>, therefore we can filter based on that <code>id</code> value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td id=\"flight\">Flight No</td>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_bs.find_all(id=\"flight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find all the elements that have links to the Florida Wikipedia page:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_input=table_bs.find_all(href=\"https://en.wikipedia.org/wiki/Florida\")\n",
    "list_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set the  <code>href</code> attribute to True, regardless of what the value is, the code finds all tags with <code>href</code> value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_bs.find_all(href=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other methods for dealing with attributes and other related methods; Check out the following <a href='https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01#css-selectors'>link</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"exer_type\">Exercise: <code>find_all</code></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the logic above, find all the elements without <code>href</code> value\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "table_bs.find_all('a', href = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```\n",
    "table_bs.find_all('a', href=False)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the soup object <code>soup</code>, find the element with the <code>id</code> attribute content set to <code>\"boldest\"</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b id=\"boldest\">Lebron James</b>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  soup.find_all(id=\"boldest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```\n",
    "soup.find_all(id=\"boldest\")\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With string you can search for strings instead of tags, where we find all the elments with Florida:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Florida', 'Florida']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_bs.find_all (string=\"Florida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>find_all()</code> method scans the entire document looking for results, it’s if you are looking for one element you can use the <code>find()</code> method to find the first element in the document. Consider the following two table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Rocket Launch </h3>\n",
       "\n",
       "<p>\n",
       "<table class='rocket'>\n",
       "  <tr>\n",
       "    <td>Flight No</td>\n",
       "    <td>Launch site</td> \n",
       "    <td>Payload mass</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>1</td>\n",
       "    <td>Florida</td>\n",
       "    <td>300 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>2</td>\n",
       "    <td>Texas</td>\n",
       "    <td>94 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>3</td>\n",
       "    <td>Florida </td>\n",
       "    <td>80 kg</td>\n",
       "  </tr>\n",
       "</table>\n",
       "</p>\n",
       "<p>\n",
       "\n",
       "<h3>Pizza Party  </h3>\n",
       "  \n",
       "    \n",
       "<table class='pizza'>\n",
       "  <tr>\n",
       "    <td>Pizza Place</td>\n",
       "    <td>Orders</td> \n",
       "    <td>Slices </td>\n",
       "   </tr>\n",
       "  <tr>\n",
       "    <td>Domino's Pizza</td>\n",
       "    <td>10</td>\n",
       "    <td>100</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Little Caesars</td>\n",
       "    <td>12</td>\n",
       "    <td >144 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Papa John's </td>\n",
       "    <td>15 </td>\n",
       "    <td>165</td>\n",
       "  </tr>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h3>Rocket Launch </h3>\n",
    "\n",
    "<p>\n",
    "<table class='rocket'>\n",
    "  <tr>\n",
    "    <td>Flight No</td>\n",
    "    <td>Launch site</td> \n",
    "    <td>Payload mass</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Florida</td>\n",
    "    <td>300 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Texas</td>\n",
    "    <td>94 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Florida </td>\n",
    "    <td>80 kg</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</p>\n",
    "<p>\n",
    "\n",
    "<h3>Pizza Party  </h3>\n",
    "  \n",
    "    \n",
    "<table class='pizza'>\n",
    "  <tr>\n",
    "    <td>Pizza Place</td>\n",
    "    <td>Orders</td> \n",
    "    <td>Slices </td>\n",
    "   </tr>\n",
    "  <tr>\n",
    "    <td>Domino's Pizza</td>\n",
    "    <td>10</td>\n",
    "    <td>100</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Little Caesars</td>\n",
    "    <td>12</td>\n",
    "    <td >144 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Papa John's </td>\n",
    "    <td>15 </td>\n",
    "    <td>165</td>\n",
    "  </tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the HTML as a Python string and assign <code>two_tables</code>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_tables=\"<h3>Rocket Launch </h3><p><table class='rocket'><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table></p><p><h3>Pizza Party  </h3><table class='pizza'><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td >144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a <code>BeautifulSoup</code> object  <code>two_tables_bs</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_tables_bs= BeautifulSoup(two_tables, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the first table using the tag name table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"rocket\"><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_tables_bs.find(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter on the class attribute to find the second table, but because class is a keyword in Python, we add an underscore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"pizza\"><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td>144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr></table>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_tables_bs.find(\"table\",class_='pizza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"DSCW\">Downloading And Scraping The Contents Of A Web Page</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Download the contents of the web page:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.ibm.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use <code>get</code> to download the contents of the webpage in text format and store in a variable called <code>data</code>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a <code>BeautifulSoup</code> object using the <code>BeautifulSoup</code> constructor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data,\"html.parser\")  # create a soup object using the variable 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape all links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ibm.com/cloud?lnk=hpUSbt1\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a',href=True):  # in html anchor/link is represented by the tag <a>\n",
    "\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape  all images  Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('img'):# in html image is represented by the tag <img>\n",
    "    print(link)\n",
    "    print(link.get('src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from HTML tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below url contains an html table with data about colors and color codes.\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/HTMLColorCodes.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to scrape a web site, you need to examine the contents, and the way data is organized on the website. Open the above url in your browser and check how many rows and columns are there in the color table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the contents of the webpage in text format and store in a variable called data\n",
    "data  = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[135], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(data,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m (soup\u001b[38;5;241m.\u001b[39mpretiffy())\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(data,\"html.parser\")\n",
    "print (soup.pretiffy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find a html table in the web page\n",
    "table = soup.find('table') # in html table is represented by the tag <table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color Name--->None\n",
      "lightsalmon--->#FFA07A\n",
      "salmon--->#FA8072\n",
      "darksalmon--->#E9967A\n",
      "lightcoral--->#F08080\n",
      "coral--->#FF7F50\n",
      "tomato--->#FF6347\n",
      "orangered--->#FF4500\n",
      "gold--->#FFD700\n",
      "orange--->#FFA500\n",
      "darkorange--->#FF8C00\n",
      "lightyellow--->#FFFFE0\n",
      "lemonchiffon--->#FFFACD\n",
      "papayawhip--->#FFEFD5\n",
      "moccasin--->#FFE4B5\n",
      "peachpuff--->#FFDAB9\n",
      "palegoldenrod--->#EEE8AA\n",
      "khaki--->#F0E68C\n",
      "darkkhaki--->#BDB76B\n",
      "yellow--->#FFFF00\n",
      "lawngreen--->#7CFC00\n",
      "chartreuse--->#7FFF00\n",
      "limegreen--->#32CD32\n",
      "lime--->#00FF00\n",
      "forestgreen--->#228B22\n",
      "green--->#008000\n",
      "powderblue--->#B0E0E6\n",
      "lightblue--->#ADD8E6\n",
      "lightskyblue--->#87CEFA\n",
      "skyblue--->#87CEEB\n",
      "deepskyblue--->#00BFFF\n",
      "lightsteelblue--->#B0C4DE\n",
      "dodgerblue--->#1E90FF\n"
     ]
    }
   ],
   "source": [
    "#Get all rows from the table\n",
    "for row in table.find_all('tr'): # in html table row is represented by the tag <tr>\n",
    "    # Get all columns in each row.\n",
    "    cols = row.find_all('td') # in html a column is represented by the tag <td>\n",
    "    color_name = cols[2].string # store the value in column 3 as color_name\n",
    "    color_code = cols[3].string # store the value in column 4 as color_code\n",
    "    print(\"{}--->{}\".format(color_name,color_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from HTML tables into a DataFrame using BeautifulSoup and Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below url contains html tables with data about world population.\n",
    "url = \"https://en.wikipedia.org/wiki/World_population\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to scrape a web site, you need to examine the contents, and the way data is organized on the website. Open the above url in your browser and check the tables on the webpage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the contents of the webpage in text format and store in a variable called data\n",
    "data  = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all html tables in the web page\n",
    "tables = soup.find_all('table') # in html table is represented by the tag <table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see how many tables were found by checking the length of the tables list\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we are looking for the `10 most densly populated countries` table, we can look through the tables list and find the right one we are look for based on the data in each table or we can search for the table name if it is in the table but this option might not always work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "for index,table in enumerate(tables):\n",
    "    if (\"10 most densely populated countries\" in str(table)):\n",
    "        table_index = index\n",
    "print(table_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can locate the table name of the table, `10 most densly populated countries`, below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"wikitable sortable\" style=\"text-align:right\">\n",
      " <caption>\n",
      "  10 most densely populated countries\n",
      "  <small>\n",
      "   (with population above 5 million)\n",
      "  </small>\n",
      "  <sup class=\"reference\" id=\"cite_ref-:10_107-0\">\n",
      "   <a href=\"#cite_note-:10-107\">\n",
      "    [102]\n",
      "   </a>\n",
      "  </sup>\n",
      " </caption>\n",
      " <tbody>\n",
      "  <tr>\n",
      "   <th scope=\"col\">\n",
      "    Rank\n",
      "   </th>\n",
      "   <th scope=\"col\">\n",
      "    Country\n",
      "   </th>\n",
      "   <th scope=\"col\">\n",
      "    Population\n",
      "   </th>\n",
      "   <th scope=\"col\">\n",
      "    Area\n",
      "    <br/>\n",
      "    <small>\n",
      "     (km\n",
      "     <sup>\n",
      "      2\n",
      "     </sup>\n",
      "     )\n",
      "    </small>\n",
      "   </th>\n",
      "   <th scope=\"col\">\n",
      "    Density\n",
      "    <br/>\n",
      "    <small>\n",
      "     (pop/km\n",
      "     <sup>\n",
      "      2\n",
      "     </sup>\n",
      "     )\n",
      "    </small>\n",
      "   </th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    1\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <span class=\"flagicon\">\n",
      "     <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "      <span>\n",
      "       <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Singapore.svg/23px-Flag_of_Singapore.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Singapore.svg/35px-Flag_of_Singapore.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Singapore.svg/45px-Flag_of_Singapore.svg.png 2x\" width=\"23\"/>\n",
      "      </span>\n",
      "     </span>\n",
      "    </span>\n",
      "    <a href=\"/wiki/Singapore\" title=\"Singapore\">\n",
      "     Singapore\n",
      "    </a>\n",
      "   </td>\n",
      "   <td>\n",
      "    5,921,231\n",
      "   </td>\n",
      "   <td>\n",
      "    719\n",
      "   </td>\n",
      "   <td>\n",
      "    8,235\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    2\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <span class=\"flagicon\">\n",
      "     <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "      <span>\n",
      "       <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"1000\" decoding=\"async\" height=\"14\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Flag_of_Bangladesh.svg/23px-Flag_of_Bangladesh.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Flag_of_Bangladesh.svg/35px-Flag_of_Bangladesh.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Flag_of_Bangladesh.svg/46px-Flag_of_Bangladesh.svg.png 2x\" width=\"23\"/>\n",
      "      </span>\n",
      "     </span>\n",
      "    </span>\n",
      "    <a href=\"/wiki/Bangladesh\" title=\"Bangladesh\">\n",
      "     Bangladesh\n",
      "    </a>\n",
      "   </td>\n",
      "   <td>\n",
      "    165,650,475\n",
      "   </td>\n",
      "   <td>\n",
      "    148,460\n",
      "   </td>\n",
      "   <td>\n",
      "    1,116\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    3\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <p>\n",
      "     <span class=\"flagicon\">\n",
      "      <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "       <span>\n",
      "        <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"1200\" decoding=\"async\" height=\"12\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/23px-Flag_of_Palestine.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/35px-Flag_of_Palestine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/46px-Flag_of_Palestine.svg.png 2x\" width=\"23\"/>\n",
      "       </span>\n",
      "      </span>\n",
      "     </span>\n",
      "     <a href=\"/wiki/State_of_Palestine\" title=\"State of Palestine\">\n",
      "      Palestine\n",
      "     </a>\n",
      "     <sup class=\"reference\" id=\"cite_ref-108\">\n",
      "      <a href=\"#cite_note-108\">\n",
      "       [note 3]\n",
      "      </a>\n",
      "     </sup>\n",
      "     <sup class=\"reference\" id=\"cite_ref-109\">\n",
      "      <a href=\"#cite_note-109\">\n",
      "       [103]\n",
      "      </a>\n",
      "     </sup>\n",
      "    </p>\n",
      "   </td>\n",
      "   <td>\n",
      "    5,223,000\n",
      "   </td>\n",
      "   <td>\n",
      "    6,025\n",
      "   </td>\n",
      "   <td>\n",
      "    867\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    4\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <span class=\"flagicon\">\n",
      "     <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "      <span>\n",
      "       <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/72/Flag_of_the_Republic_of_China.svg/23px-Flag_of_the_Republic_of_China.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/72/Flag_of_the_Republic_of_China.svg/35px-Flag_of_the_Republic_of_China.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/72/Flag_of_the_Republic_of_China.svg/45px-Flag_of_the_Republic_of_China.svg.png 2x\" width=\"23\"/>\n",
      "      </span>\n",
      "     </span>\n",
      "    </span>\n",
      "    <a href=\"/wiki/Taiwan\" title=\"Taiwan\">\n",
      "     Taiwan\n",
      "    </a>\n",
      "    <sup class=\"reference\" id=\"cite_ref-110\">\n",
      "     <a href=\"#cite_note-110\">\n",
      "      [note 4]\n",
      "     </a>\n",
      "    </sup>\n",
      "   </td>\n",
      "   <td>\n",
      "    23,580,712\n",
      "   </td>\n",
      "   <td>\n",
      "    35,980\n",
      "   </td>\n",
      "   <td>\n",
      "    655\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    5\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <span class=\"flagicon\">\n",
      "     <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "      <span>\n",
      "       <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/0/09/Flag_of_South_Korea.svg/23px-Flag_of_South_Korea.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/0/09/Flag_of_South_Korea.svg/35px-Flag_of_South_Korea.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/09/Flag_of_South_Korea.svg/45px-Flag_of_South_Korea.svg.png 2x\" width=\"23\"/>\n",
      "      </span>\n",
      "     </span>\n",
      "    </span>\n",
      "    <a href=\"/wiki/South_Korea\" title=\"South Korea\">\n",
      "     South Korea\n",
      "    </a>\n",
      "   </td>\n",
      "   <td>\n",
      "    51,844,834\n",
      "   </td>\n",
      "   <td>\n",
      "    99,720\n",
      "   </td>\n",
      "   <td>\n",
      "    520\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    6\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <span class=\"flagicon\">\n",
      "     <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "      <span>\n",
      "       <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/5/59/Flag_of_Lebanon.svg/23px-Flag_of_Lebanon.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/5/59/Flag_of_Lebanon.svg/35px-Flag_of_Lebanon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/59/Flag_of_Lebanon.svg/45px-Flag_of_Lebanon.svg.png 2x\" width=\"23\"/>\n",
      "      </span>\n",
      "     </span>\n",
      "    </span>\n",
      "    <a href=\"/wiki/Lebanon\" title=\"Lebanon\">\n",
      "     Lebanon\n",
      "    </a>\n",
      "   </td>\n",
      "   <td>\n",
      "    5,296,814\n",
      "   </td>\n",
      "   <td>\n",
      "    10,400\n",
      "   </td>\n",
      "   <td>\n",
      "    509\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    7\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <span class=\"flagicon\">\n",
      "     <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "      <span>\n",
      "       <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Rwanda.svg/23px-Flag_of_Rwanda.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Rwanda.svg/35px-Flag_of_Rwanda.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Rwanda.svg/45px-Flag_of_Rwanda.svg.png 2x\" width=\"23\"/>\n",
      "      </span>\n",
      "     </span>\n",
      "    </span>\n",
      "    <a href=\"/wiki/Rwanda\" title=\"Rwanda\">\n",
      "     Rwanda\n",
      "    </a>\n",
      "   </td>\n",
      "   <td>\n",
      "    13,173,730\n",
      "   </td>\n",
      "   <td>\n",
      "    26,338\n",
      "   </td>\n",
      "   <td>\n",
      "    500\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    8\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <span class=\"flagicon\">\n",
      "     <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "      <span>\n",
      "       <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"1000\" decoding=\"async\" height=\"14\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/5/50/Flag_of_Burundi.svg/23px-Flag_of_Burundi.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/5/50/Flag_of_Burundi.svg/35px-Flag_of_Burundi.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/50/Flag_of_Burundi.svg/46px-Flag_of_Burundi.svg.png 2x\" width=\"23\"/>\n",
      "      </span>\n",
      "     </span>\n",
      "    </span>\n",
      "    <a href=\"/wiki/Burundi\" title=\"Burundi\">\n",
      "     Burundi\n",
      "    </a>\n",
      "   </td>\n",
      "   <td>\n",
      "    12,696,478\n",
      "   </td>\n",
      "   <td>\n",
      "    27,830\n",
      "   </td>\n",
      "   <td>\n",
      "    456\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    9\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <span class=\"flagicon\">\n",
      "     <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "      <span>\n",
      "       <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/4/41/Flag_of_India.svg/23px-Flag_of_India.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/4/41/Flag_of_India.svg/35px-Flag_of_India.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/41/Flag_of_India.svg/45px-Flag_of_India.svg.png 2x\" width=\"23\"/>\n",
      "      </span>\n",
      "     </span>\n",
      "    </span>\n",
      "    <a href=\"/wiki/India\" title=\"India\">\n",
      "     India\n",
      "    </a>\n",
      "   </td>\n",
      "   <td>\n",
      "    1,389,637,446\n",
      "   </td>\n",
      "   <td>\n",
      "    3,287,263\n",
      "   </td>\n",
      "   <td>\n",
      "    423\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    10\n",
      "   </td>\n",
      "   <td align=\"left\">\n",
      "    <span class=\"flagicon\">\n",
      "     <span class=\"mw-image-border\" typeof=\"mw:File\">\n",
      "      <span>\n",
      "       <img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/23px-Flag_of_the_Netherlands.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/35px-Flag_of_the_Netherlands.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/45px-Flag_of_the_Netherlands.svg.png 2x\" width=\"23\"/>\n",
      "      </span>\n",
      "     </span>\n",
      "    </span>\n",
      "    <a href=\"/wiki/Netherlands\" title=\"Netherlands\">\n",
      "     Netherlands\n",
      "    </a>\n",
      "   </td>\n",
      "   <td>\n",
      "    17,400,824\n",
      "   </td>\n",
      "   <td>\n",
      "    41,543\n",
      "   </td>\n",
      "   <td>\n",
      "    419\n",
      "   </td>\n",
      "  </tr>\n",
      " </tbody>\n",
      "</table>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tables[table_index].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17648\\763445193.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mcountry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mpopulation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0marea\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdensity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mpopulation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulation_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Rank\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Country\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcountry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Population\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Area\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0marea\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Density\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdensity\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpopulation_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "population_data = pd.DataFrame(columns=[\"Rank\", \"Country\", \"Population\", \"Area\", \"Density\"])\n",
    "\n",
    "for row in tables[table_index].tbody.find_all(\"tr\"):\n",
    "    col = row.find_all(\"td\")\n",
    "    if (col != []):\n",
    "        rank = col[0].text\n",
    "        country = col[1].text\n",
    "        population = col[2].text.strip()\n",
    "        area = col[3].text.strip()\n",
    "        density = col[4].text.strip()\n",
    "        population_data = population_data.append({\"Rank\":rank, \"Country\":country, \"Population\":population, \"Area\":area, \"Density\":density}, ignore_index=True)\n",
    "   \n",
    "population_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from HTML tables into a DataFrame using BeautifulSoup and read_html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same `url`, `data`, `soup`, and `tables` object as in the last section we can use the `read_html` function to create a DataFrame.\n",
    "\n",
    "Remember the table we need is located in `tables[table_index]`\n",
    "\n",
    "We can now use the `pandas` function `read_html` and give it the string version of the table as well as the `flavor` which is the parsing engine `bs4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_html(\u001b[38;5;28mstr\u001b[39m(tables[\u001b[38;5;241m5\u001b[39m]), flavor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:1212\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend)\u001b[0m\n\u001b[0;32m   1208\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m   1210\u001b[0m io \u001b[38;5;241m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\n\u001b[0;32m   1213\u001b[0m     flavor\u001b[38;5;241m=\u001b[39mflavor,\n\u001b[0;32m   1214\u001b[0m     io\u001b[38;5;241m=\u001b[39mio,\n\u001b[0;32m   1215\u001b[0m     match\u001b[38;5;241m=\u001b[39mmatch,\n\u001b[0;32m   1216\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   1217\u001b[0m     index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   1218\u001b[0m     skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[0;32m   1219\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m   1220\u001b[0m     thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[0;32m   1221\u001b[0m     attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1222\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1223\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   1224\u001b[0m     converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[0;32m   1225\u001b[0m     na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[0;32m   1226\u001b[0m     keep_default_na\u001b[38;5;241m=\u001b[39mkeep_default_na,\n\u001b[0;32m   1227\u001b[0m     displayed_only\u001b[38;5;241m=\u001b[39mdisplayed_only,\n\u001b[0;32m   1228\u001b[0m     extract_links\u001b[38;5;241m=\u001b[39mextract_links,\n\u001b[0;32m   1229\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1230\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:1001\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n\u001b[0;32m   1003\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:981\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 981\u001b[0m     tables \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mparse_tables()\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:257\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_doc(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:674\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._build_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m     udoc \u001b[38;5;241m=\u001b[39m bdoc\n\u001b[0;32m    672\u001b[0m     from_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding\n\u001b[1;32m--> 674\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(udoc, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml5lib\u001b[39m\u001b[38;5;124m\"\u001b[39m, from_encoding\u001b[38;5;241m=\u001b[39mfrom_encoding)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m br \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    677\u001b[0m     br\u001b[38;5;241m.\u001b[39mreplace_with(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m br\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\__init__.py:250\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m     if builder_class is None:\n\u001b[0;32m    245\u001b[0m         raise FeatureNotFound(\n\u001b[0;32m    246\u001b[0m             \"Couldn't find a tree builder with the features you \"\n\u001b[0;32m    247\u001b[0m             \"requested: %s. Do you need to install a parser library?\"\n\u001b[0;32m    248\u001b[0m             % \",\".join(features))\n\u001b[1;32m--> 250\u001b[0m # At this point either we have a TreeBuilder instance in\n\u001b[0;32m    251\u001b[0m # builder, or we have a builder_class that we can instantiate\n\u001b[0;32m    252\u001b[0m # with the remaining **kwargs.\n\u001b[0;32m    253\u001b[0m if builder is None:\n\u001b[0;32m    254\u001b[0m     builder = builder_class(**kwargs)\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "pd.read_html(str(tables[5]), flavor='bs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `read_html` always returns a list of DataFrames so we must pick the one we want out of the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m population_data_read_html \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(\u001b[38;5;28mstr\u001b[39m(tables[\u001b[38;5;241m5\u001b[39m]), flavor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m population_data_read_html\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:1212\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend)\u001b[0m\n\u001b[0;32m   1208\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m   1210\u001b[0m io \u001b[38;5;241m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\n\u001b[0;32m   1213\u001b[0m     flavor\u001b[38;5;241m=\u001b[39mflavor,\n\u001b[0;32m   1214\u001b[0m     io\u001b[38;5;241m=\u001b[39mio,\n\u001b[0;32m   1215\u001b[0m     match\u001b[38;5;241m=\u001b[39mmatch,\n\u001b[0;32m   1216\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   1217\u001b[0m     index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   1218\u001b[0m     skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[0;32m   1219\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m   1220\u001b[0m     thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[0;32m   1221\u001b[0m     attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1222\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1223\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   1224\u001b[0m     converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[0;32m   1225\u001b[0m     na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[0;32m   1226\u001b[0m     keep_default_na\u001b[38;5;241m=\u001b[39mkeep_default_na,\n\u001b[0;32m   1227\u001b[0m     displayed_only\u001b[38;5;241m=\u001b[39mdisplayed_only,\n\u001b[0;32m   1228\u001b[0m     extract_links\u001b[38;5;241m=\u001b[39mextract_links,\n\u001b[0;32m   1229\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1230\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:1001\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n\u001b[0;32m   1003\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:981\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 981\u001b[0m     tables \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mparse_tables()\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:257\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_doc(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:674\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._build_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m     udoc \u001b[38;5;241m=\u001b[39m bdoc\n\u001b[0;32m    672\u001b[0m     from_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding\n\u001b[1;32m--> 674\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(udoc, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml5lib\u001b[39m\u001b[38;5;124m\"\u001b[39m, from_encoding\u001b[38;5;241m=\u001b[39mfrom_encoding)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m br \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    677\u001b[0m     br\u001b[38;5;241m.\u001b[39mreplace_with(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m br\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\__init__.py:250\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m     if builder_class is None:\n\u001b[0;32m    245\u001b[0m         raise FeatureNotFound(\n\u001b[0;32m    246\u001b[0m             \"Couldn't find a tree builder with the features you \"\n\u001b[0;32m    247\u001b[0m             \"requested: %s. Do you need to install a parser library?\"\n\u001b[0;32m    248\u001b[0m             % \",\".join(features))\n\u001b[1;32m--> 250\u001b[0m # At this point either we have a TreeBuilder instance in\n\u001b[0;32m    251\u001b[0m # builder, or we have a builder_class that we can instantiate\n\u001b[0;32m    252\u001b[0m # with the remaining **kwargs.\n\u001b[0;32m    253\u001b[0m if builder is None:\n\u001b[0;32m    254\u001b[0m     builder = builder_class(**kwargs)\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "population_data_read_html = pd.read_html(str(tables[5]), flavor='bs4')[0]\n",
    "\n",
    "population_data_read_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from HTML tables into a DataFrame using read_html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `read_html` function to directly get DataFrames from a `url`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m     h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1349\u001b[0m               encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1331\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m \n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1451\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1451\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:945\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    944\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[1;32m--> 945\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection(\n\u001b[0;32m    946\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address)\n\u001b[0;32m    947\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:827\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    826\u001b[0m exceptions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m getaddrinfo(host, port, \u001b[38;5;241m0\u001b[39m, SOCK_STREAM):\n\u001b[0;32m    828\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataframe_list \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(url, flavor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:1212\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend)\u001b[0m\n\u001b[0;32m   1208\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m   1210\u001b[0m io \u001b[38;5;241m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\n\u001b[0;32m   1213\u001b[0m     flavor\u001b[38;5;241m=\u001b[39mflavor,\n\u001b[0;32m   1214\u001b[0m     io\u001b[38;5;241m=\u001b[39mio,\n\u001b[0;32m   1215\u001b[0m     match\u001b[38;5;241m=\u001b[39mmatch,\n\u001b[0;32m   1216\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   1217\u001b[0m     index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   1218\u001b[0m     skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[0;32m   1219\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m   1220\u001b[0m     thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[0;32m   1221\u001b[0m     attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1222\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1223\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   1224\u001b[0m     converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[0;32m   1225\u001b[0m     na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[0;32m   1226\u001b[0m     keep_default_na\u001b[38;5;241m=\u001b[39mkeep_default_na,\n\u001b[0;32m   1227\u001b[0m     displayed_only\u001b[38;5;241m=\u001b[39mdisplayed_only,\n\u001b[0;32m   1228\u001b[0m     extract_links\u001b[38;5;241m=\u001b[39mextract_links,\n\u001b[0;32m   1229\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1230\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:981\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 981\u001b[0m     tables \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mparse_tables()\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:257\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_doc(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:666\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._build_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_doc\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m--> 666\u001b[0m     bdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_build_doc()\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bdoc, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m         udoc \u001b[38;5;241m=\u001b[39m bdoc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:658\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._setup_build_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_build_doc\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 658\u001b[0m     raw_text \u001b[38;5;241m=\u001b[39m _read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_text:\n\u001b[0;32m    660\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo text parsed from document: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\html.py:155\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(obj, encoding)\u001b[0m\n\u001b[0;32m    149\u001b[0m text: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    151\u001b[0m     is_url(obj)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file_exists(obj))\n\u001b[0;32m    154\u001b[0m ):\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_handle(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    156\u001b[0m         text \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:716\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    713\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 716\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    717\u001b[0m     path_or_buf,\n\u001b[0;32m    718\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    719\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    720\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    721\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    725\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:368\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    367\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    369\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:270\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    537\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[0;32m   1392\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context, check_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_hostname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1349\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "source": [
    "dataframe_list = pd.read_html(url, flavor='bs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are 25 DataFrames just like when we used `find_all` on the `soup` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can pick the DataFrame we need out of the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_list[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `match` parameter to select the specific table we want. If the table contains a string matching the text it will be read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_html(url, match=\"10 most densely populated countries\", flavor='bs4')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ramesh Sannareddy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rav Ahuja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date (YYYY-MM-DD) | Version | Changed By                                               | Change Description |\n",
    "| ----------------- | ------- | -------------------------------------------------------- | ------------------ |\n",
    "| 2021-08-04        | 0.2     | Made changes to markdown of nextsibling                  |                    |\n",
    "| 2020-10-17        | 0.1     | Joseph Santarcangelo  Created initial version of the lab |                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork-19487395&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
